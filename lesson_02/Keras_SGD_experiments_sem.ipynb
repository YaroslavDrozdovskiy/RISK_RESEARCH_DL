{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация и градиентные спуски :) \n",
    "\n",
    "В этой тетрадке мы попробуем немного посмотреть на то, как работают разные градиентные спуски. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings         # чтобы никто не мешал бесчинствам с кодом\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Выборка\n",
    "\n",
    "Делать всё это мы будем на животных. Ежегодно около 7.6 миллионов бедных животных в США оказываются в приютах. Часть из них находит себе новую семью, часть возвращается к старому (бывает, что питомец потерялся и его нашли на улице), а часть погибает. Ужегодно усыпляется около 2.7 млн. собак и кошек.  \n",
    "\n",
    "Используя датасет с входной информацией (цвет, пол, возраст и т.п.) из одного из приютов, мы попытаемся спрогнозировать что произойдёт с новыми животными, которые попадут в этот приют. Данные, используемые в тетрадке уже были предварительно обработаны и приведены в удобную для построения моделей форму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_cat.csv', sep = '\\t', index_col=0)\n",
    "target = pd.read_csv('y_cat.csv', sep = '\\t', index_col=0, names=['status'])\n",
    "\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете находится около 27 тысяч наблюдений и 39 регрессоров. Посмотрим на то как выглядит распределение того, что произошло со зверятами по особям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что классы несбалансированы. Попробуем оставит четыре класса и объединить класс умерших животных с классом животных, которых усыпили. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.iloc[:, :]\n",
    "target[target == 'Died'] = 'Euthanasia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируем классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём выборку на тренировочную и тестовую. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state = 42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Архитектурка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "keras = tf.keras\n",
    "\n",
    "from keras.models  import Sequential   # Для сборки модели\n",
    "import keras.layers as L               # Разные слои\n",
    "import keras.optimizers as opt         # Оптимизаторы\n",
    "\n",
    "# Для преобразования y в категориальный формат \n",
    "from keras.utils import to_categorical  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для картинок с прошлой пары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories, key='loss', start=0):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch[start:], history.history['val_'+key][start:],\n",
    "                       #'--', \n",
    "                       label=name.title()+' Val')\n",
    "            #plt.plot(history.epoch[start:], history.history[key][start:], color=val[0].get_color(),\n",
    "            #     label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([start, max(history.epoch)])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переменную $y$ для успешного обучения сетки нужно будет перевести в матрицу из дамми-переменных с помощью команды `to_categorical`.  У нас тут как-бы __Softmax.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "def fn_model() : \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(L.Dense(25, input_dim = X_train.shape[1], kernel_initializer='random_normal'))\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "    \n",
    "    model.add(L.Dense(25, kernel_initializer='random_normal'))\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "    \n",
    "    # На выходе мы должны получить вероятности того, что объект относится к разным классам \n",
    "    # Сделать такое преобразование позволяет softmax как функция активации\n",
    "    # На выход будет идти 4 вероятности по числу классов\n",
    "    model.add(L.Dense(4, activation='softmax', kernel_initializer = 'random_normal'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Оптимизация \n",
    "\n",
    "### SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model1 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "sgd = opt.SGD(lr=learning_rate, momentum=0.0, nesterov=False)\n",
    "\n",
    "# собрали модель\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('baseline', history1), ],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model2 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "sgd = opt.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "# собрали модель\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('SGD', history1), \n",
    "              ('momentum nesterov SGD', history2),],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model3 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "sgd = opt.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08)\n",
    "\n",
    "# собрали модель\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('SGD', history1), \n",
    "              ('momentum nesterov SGD', history2),\n",
    "              ('RMSprop', history3),],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model4 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "sgd = opt.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# собрали модель\n",
    "model4.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history4 = model4.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('SGD', history1), \n",
    "              ('momentum nesterov SGD', history2),\n",
    "              ('RMSprop', history3),\n",
    "              ('ADAM', history4),\n",
    "             ],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Стратегии с постепенным понижением lr \n",
    "\n",
    "![](https://raw.githubusercontent.com/FUlyankin/neural_nets_econ/master/2019/sem_2/ahaha.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler # колбэк для корректировки скорости обучения\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# Хотим наблюдать за тем как меняется скорость каждую эпоху, для этого делаем свой колбэк!\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(lr_scheduler(len(self.losses)))\n",
    "       \n",
    "    \n",
    "# функция для картинок, чтобы видеть как скорость обученя меняется от эпохи к эпохе\n",
    "def plot_learning_rate(loss_history):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,epochs+1),loss_history.lr,label='learning rate')\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.xlim([1,epochs+1])\n",
    "    plt.ylabel(\"learning rate\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Learning rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стартовую скорость обучения специально берём большой! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.1  # берём lr намеренно большим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем архитектурку! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# Первая простенькая моделька \n",
    "model5 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "#sgd = opt.SGD(lr=INIT_LR, momentum=0.0, nesterov=False)\n",
    "sgd = opt.Adam(lr=INIT_LR, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# собрали модель\n",
    "model5.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Стратегия для понижения скорости\n",
    "def lr_scheduler(epoch):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 50.0\n",
    "    lrate = INIT_LR * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# список из наших колбэков \n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "history5 = model5.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0,\n",
    "                      callbacks=callbacks_list,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим как скорость обучения по немного понижалась\n",
    "plot_learning_rate(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('SGD', history1), \n",
    "              ('momentum nesterov SGD', history2),\n",
    "              ('RMSprop', history3),\n",
    "              ('ADAM', history4),\n",
    "              ('ADAM_step', history5)\n",
    "             ],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем ещё вариант!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первая простенькая моделька \n",
    "model6 = fn_model()\n",
    "\n",
    "# SGD optimizer\n",
    "sgd = opt.Adam(lr=INIT_LR, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# собрали модель\n",
    "model6.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Стратегия для понижения скорости\n",
    "def lr_scheduler(epoch):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    cur_lr = INIT_LR * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
    "    if cur_lr > 0.001:\n",
    "        lrate = INIT_LR * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
    "    else: \n",
    "        lrate = 0.001\n",
    "    return lrate\n",
    "\n",
    "# список из наших колбэков \n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(lr_scheduler)\n",
    "callbacks_list = [loss_history, lrate]\n",
    "\n",
    "history6 = model6.fit(X_train, y_train, \n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0,\n",
    "                      callbacks=callbacks_list,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('SGD', history1), \n",
    "              ('momentum nesterov SGD', history2),\n",
    "              ('RMSprop', history3),\n",
    "              ('ADAM', history4),\n",
    "              ('ADAM_step', history5),\n",
    "              ('ADAM_step_floor', history6),\n",
    "             ],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Авторские права и почиташки \n",
    "\n",
    "* Для создания тетрадки использовал [вот этот мануал](https://github.com/sukilau/Ziff-deep-learning/blob/master/3-CIFAR10-lrate/CIFAR10-lrate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
